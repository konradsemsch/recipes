% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/discretize_tree.R
\name{step_discretize_tree}
\alias{step_discretize_tree}
\title{Discretize numeric variables with XgBoost}
\usage{
step_discretize_tree(recipe, ..., role = NA, trained = FALSE,
  outcome = NULL, learn_rate = 0.3, num_breaks = 10,
  tree_depth = 1, rules = NULL, prefix = "bin_tree", skip = FALSE,
  id = rand_id("discretize_tree"))
}
\arguments{
\item{recipe}{A recipe object. The step will be added to the sequence of
operations for this recipe.}

\item{...}{One or more selector functions to choose which variables are
affected by the step. See \code{\link[=selections]{selections()}} for more details.}

\item{role}{Defaults to \code{"predictor"}.}

\item{trained}{A logical to indicate if the quantities for preprocessing
have been estimated.}

\item{outcome}{A call to \code{vars} to specify which variable is
used as the outcome to train XbBoost models in order to discretize explanatory variables.}

\item{learn_rate}{The rate at which the boosting algorithm adapts from iteration-to-iteration.
Corresponds to 'eta' in the \pkg{xgboost} package. Defaults to 0.3.}

\item{num_breaks}{The maximum number of discrete bins to bucket continuous features.
Corresponds to 'max_bin' in the \pkg{xgboost} package. Defaults to 10.}

\item{tree_depth}{The maximum depth of the tree (i.e. number of splits).
Corresponds to 'max_depth' in the \pkg{xgboost} package. Defaults to 1.}

\item{rules}{The splitting rules of the best XgBoost tree to retain for each variable.}

\item{prefix}{A character string that will be the prefix to the resulting new variables.
Defaults to "xgb_bin".}
}
\value{
An updated version of \code{recipe} with the
new step added to the sequence of existing steps (if any).
}
\description{
\code{step_discretize_tree} creates a \emph{specification} of a recipe
step that will discretize numeric data (e.g. integeres or doubles)
into bins in a supervised way using an XgBoost model.
}
\details{
step_discretize_tree creates non-uniform bins from numerical
variables by utilizing the information about the outcome
variable and applying the xgboost model. It is advised to impute
missing values before this step. This step is intented to be
used particularly with linear models because thanks to creating
non-uniform bins it becomes easier to learn non-linear patterns
from the data.

The best selection of buckets for each variable is selected using
an internal early stopping scheme implemented in the \pkg{xgboost}
package, which makes this discretization method prone to overfitting.

The pre-defined values of the underlying xgboost learns should give
good and reasonably complex results. However, if one wishes to tune them
the recommended path would be to first start with changing the value
of 'num_breaks' to e.g.: 20 or 30. If that doesn't give satisfactory results
one could experiment with increasing the 'tree_depth' parameter.

This step requires the \pkg{xgboost} package. If not installed, the
step will stop with a note about installing the package.

Note that the original variables will be replaced with the new bins.
The new variables will have names that begin with \code{prefix} followed
by their original name.
}
\examples{
data(credit_data)
library(rsample)

split <- initial_split(credit_data, strata = "Status")

credit_data_tr <- training(split)
credit_data_te <- testing(split)

xgb_rec <- recipe(Status ~ ., data = credit_data_tr) \%>\%
  step_medianimpute(all_numeric()) \%>\%
  step_discretize_tree(all_numeric(), outcome = "Status")

xgb_rec <- prep(xgb_rec, training = credit_data_tr, retain = TRUE)

xgb_test_bins <- bake(xgb_rec, credit_data_te)
}
\seealso{
\code{\link[=recipe]{recipe()}} \code{\link[=prep.recipe]{prep.recipe()}} \code{\link[=bake.recipe]{bake.recipe()}}
}
\concept{discretization}
\concept{factors}
\concept{preprocessing}
\concept{xgboost}
\keyword{binning}
